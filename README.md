# neural-code-generation-moe
This paper explores the efficacy of the Mixture-of-Experts (MoE) framework in the realm of code generation, presenting a compelling performance compared to monolithic model architecture

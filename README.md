# neural-code-generation-moe
This paper explores the efficacy of the Mixture-of-Experts (MoE) framework in the realm of code generation, presenting a compelling performance compared to monolithic model architecture

<img width="961" alt="image" src="https://github.com/Santhoshkumar-p/neural-code-generation-moe/assets/24734488/5bb27578-ffc9-494d-b2c2-169ed0258972">

<img width="938" alt="image" src="https://github.com/Santhoshkumar-p/neural-code-generation-moe/assets/24734488/fa2428c5-3e60-4524-955a-409d162bfff2">



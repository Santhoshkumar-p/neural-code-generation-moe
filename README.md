# neural-code-generation-moe
This paper explores the efficacy of the Mixture-of-Experts (MoE) framework in the realm of code generation, presenting a compelling performance compared to monolithic model architecture

<img width="961" alt="image" src="https://github.com/Santhoshkumar-p/neural-code-generation-moe/assets/24734488/5bb27578-ffc9-494d-b2c2-169ed0258972">

